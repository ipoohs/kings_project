---
title: "Project-based master's Dissertation"
author: "Fuangrit Srinual"
date: '2022-07-01'
output: word_document
---

Research question - To what extent are fake and real news differ in terms of text analysis? 

This project will execute text analysis processes in two main steps, which are sentiment analysis and word frequency.

The datasets utilised in this project has been retrieved from https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset by CLÃ‰MENT BISAILLON


```{r}
fake <- read.csv("~/Documents/2021KCLsem2/Dissertation/fakenewsdata/Fake.csv")
true <- read.csv("~/Documents/2021KCLsem2/Dissertation/fakenewsdata/True.csv")
```


```{r}
library(syuzhet)
library(textdata)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggpubr)
library(NLP)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(ggplot2)
library(gofastr)
library(dplyr)
library(tidytext)
library(tidyr)
library(stringr)
library(textstem)
library(quanteda.textstats)
library(stargazer)
```


```{r}
dim(fake)
dim(true)
```


```{r}
fake_token = data.frame(text=fake$text, stringsAsFactors = FALSE) %>% unnest_tokens(word, text)
fake_senti = inner_join(fake_token, get_sentiments("nrc")) %>%
  dplyr::count(sentiment)
fake_senti$percent = (fake_senti$n/sum(fake_senti$n))*100
fake_senti_plot <- ggplot(fake_senti, aes(sentiment, percent)) +   
        geom_bar(aes(fill = sentiment), position = 'dodge', stat = 'identity')+ 
  coord_flip() +
        theme(legend.position = 'none', plot.title = element_text(size=18, face = 'bold'),
              axis.text=element_text(size=16),
              axis.title=element_text(size=14,face="bold"))



true_token = data.frame(text=true$text, stringsAsFactors = FALSE) %>% unnest_tokens(word, text)
true_senti = inner_join(true_token, get_sentiments("nrc")) %>%
  count(sentiment)
true_senti$percent = (true_senti$n/sum(true_senti$n))*100
true_senti_plot <- ggplot(true_senti, aes(sentiment, percent)) +   
        geom_bar(aes(fill = sentiment), position = 'dodge', stat = 'identity')+
  coord_flip() +
        theme(legend.position = 'none', plot.title = element_text(size=18),
              axis.text=element_text(size=16),
              axis.title=element_text(size=14,face="bold"))


ggarrange(fake_senti_plot, true_senti_plot, 
          labels = c("Fake", "True"),
          ncol = 2, nrow = 1)
```



Other approaches for a sentiment analysis


```{r}
fake_senti_bing = inner_join(fake_token, get_sentiments("bing")) %>%
  dplyr::count(sentiment)
fake_senti_bing$percent = (fake_senti_bing$n/sum(fake_senti_bing$n))*100
fake_senti_bing_plot <- ggplot(fake_senti_bing, aes(sentiment, percent)) +   
        geom_bar(aes(fill = sentiment), position = 'dodge', stat = 'identity')+ 
  coord_flip() +
        theme(legend.position = 'none', plot.title = element_text(size=18, face = 'bold'),
              axis.text=element_text(size=16),
              axis.title=element_text(size=14,face="bold"))



true_senti_bing = inner_join(true_token, get_sentiments("bing")) %>%
  count(sentiment)
true_senti_bing$percent = (true_senti_bing$n/sum(true_senti_bing$n))*100
true_senti_bing_plot <- ggplot(true_senti_bing, aes(sentiment, percent)) +   
        geom_bar(aes(fill = sentiment), position = 'dodge', stat = 'identity')+
  coord_flip() +
        theme(legend.position = 'none', plot.title = element_text(size=18),
              axis.text=element_text(size=16),
              axis.title=element_text(size=14,face="bold"))


ggarrange(fake_senti_bing_plot, true_senti_bing_plot, 
          labels = c("Fake", "True"),
          ncol = 2, nrow = 1)
```


```{r}
fake_senti_bing$percent
true_senti_bing$percent
```



```{r}
fake_senti_afinn = inner_join(fake_token, get_sentiments("afinn")) %>%
  dplyr::count(value)
fake_senti_afinn$percent = (fake_senti_afinn$n/sum(fake_senti_afinn$n))*100
fake_senti_afinn_plot <- ggplot(fake_senti_afinn, aes(value, percent)) +   
        geom_bar(aes(fill = value), position = 'dodge', stat = 'identity')+ 
  coord_flip() +
        theme(legend.position = 'none', plot.title = element_text(size=18, face = 'bold'),
              axis.text=element_text(size=16),
              axis.title=element_text(size=14,face="bold"))


true_senti_afinn = inner_join(true_token, get_sentiments("afinn")) %>%
  dplyr::count(value)
true_senti_afinn$percent = (true_senti_afinn$n/sum(true_senti_afinn$n))*100
true_senti_afinn_plot <- ggplot(true_senti_afinn, aes(value, percent)) +   
        geom_bar(aes(fill = value), position = 'dodge', stat = 'identity')+
  coord_flip() +
        theme(legend.position = 'none', plot.title = element_text(size=18),
              axis.text=element_text(size=16),
              axis.title=element_text(size=14,face="bold"))


ggarrange(fake_senti_afinn_plot, true_senti_afinn_plot, 
          labels = c("Fake", "True"),
          ncol = 2, nrow = 1)
```


```{r}
fake_senti_loughran = inner_join(fake_token, get_sentiments("loughran")) %>% dplyr::count(sentiment)
fake_senti_loughran$percent = (fake_senti_loughran$n/sum(fake_senti_loughran$n))*100
fake_senti_loughran_plot <- ggplot(fake_senti_loughran, aes(sentiment, percent)) +   
        geom_bar(aes(fill = sentiment), position = 'dodge', stat = 'identity')+ 
  coord_flip() +
        theme(legend.position = 'none', plot.title = element_text(size=18, face = 'bold'),
              axis.text=element_text(size=16),
              axis.title=element_text(size=14,face="bold"))


true_senti_loughran = inner_join(true_token, get_sentiments("loughran")) %>%
  dplyr::count(sentiment)
true_senti_loughran$percent = (true_senti_loughran$n/sum(true_senti_loughran$n))*100
true_senti_loughran_plot <- ggplot(true_senti_loughran, aes(sentiment, percent)) +   
        geom_bar(aes(fill = sentiment), position = 'dodge', stat = 'identity')+
  coord_flip() +
        theme(legend.position = 'none', plot.title = element_text(size=18),
              axis.text=element_text(size=16),
              axis.title=element_text(size=14,face="bold"))


ggarrange(fake_senti_loughran_plot, true_senti_loughran_plot, 
          labels = c("Fake", "True"),
          ncol = 2, nrow = 1)
```


Data cleaning


```{r}
fake$cleaned_text <- tolower(fake$text)# Lowercase
fake$cleaned_text <- lemmatize_strings(fake$cleaned_text) #Lemmatization
fake$cleaned_text <- gsub("[[:punct:]]", "", fake$cleaned_text) # Remove punctuation
fake$cleaned_text <- gsub("[[:digit:]]", "", fake$cleaned_text) # Remove numbers
fake$cleaned_text <- gsub("\\s+", " ", str_trim(fake$cleaned_text))
fake$cleaned_text <- removeWords(fake$cleaned_text, stopwords("en"))
```


```{r}
all_stops <- c("just", "might", "ll", "t", "ve","said", "will","can", "also","get", "like", "say", "s",stopwords("en"))

fake$cleaned_text <- removeWords(fake$cleaned_text, all_stops)
```


```{r}
true$cleaned_text <- tolower(true$text) # Lowercase
true$cleaned_text <- lemmatize_strings(true$cleaned_text) #Lemmatization
true$cleaned_text <- gsub("[[:punct:]]", "", true$cleaned_text) # Remove punctuation
true$cleaned_text <- gsub("[[:digit:]]", "", true$cleaned_text) # Remove numbers
true$cleaned_text <- gsub("\\s+", " ", str_trim(true$cleaned_text))
true$cleaned_text <- removeWords(true$cleaned_text, all_stops)

```


```{r}
data_cleaning <- function(c) {
  c <- Corpus(VectorSource(c$cleaned_text))                 
    removeURL <- function(x) gsub('http[[:alnum:]]*','', x)
  c<- tm_map(c, content_transformer(removeURL))
  c <- tm_map(c, stripWhitespace)
  c <- tm_map(c, content_transformer(function(s)
  {
    gsub(pattern = '[^a-zA-Z0-9\\s]+',
    x = s,
    replacement = " " ,
     ignore.case = TRUE,
    perl = TRUE)
  } ))
    removeNonAscii <- function(x) textclean::replace_non_ascii(x)
  c <- tm_map(c, content_transformer(removeNonAscii))
  return(c)
  }
```


```{r}
fake_text_clean <- data_cleaning(fake)
true_text_clean <- data_cleaning(true)
```


Word frequency


```{r}
set.seed(555)
palet <- brewer.pal(8, 'Dark2')
wordcloud(fake_text_clean, min.freq = 4000, scale = c(4.5, 0.3) , random.order = F, col = palet)
```


```{r}
set.seed(555)
palet <- brewer.pal(8, 'Dark2')
wordcloud(true_text_clean, min.freq = 4000, scale = c(4.5, 0.3) , random.order = F, col = palet)
```


```{r}
fake_dtm <- DocumentTermMatrix(fake_text_clean, control = list(stopwords = TRUE, removePunctuation = TRUE, stemming = FALSE, removeNumbers = TRUE))

true_dtm <- DocumentTermMatrix(true_text_clean, control = list(stopwords = TRUE, removePunctuation = TRUE, stemming = FALSE, removeNumbers = TRUE))
```


```{r}
fake_doc.length <- apply(fake_dtm, 1, sum)
fake_dtm_frequency <- colSums(as.matrix(fake_dtm))
fake_dtm <- fake_dtm[fake_doc.length > 0,]
fake_dtm
```


```{r}
true_doc.length <- apply(true_dtm, 1, sum)
true_dtm_frequency <- colSums(as.matrix(true_dtm))
true_dtm <- true_dtm[true_doc.length > 0,]
true_dtm
```


```{r}
fake_dtm_order <- order(fake_dtm_frequency, decreasing = TRUE)
fake_order <- fake_dtm_frequency[head(fake_dtm_order, n = 20)]
fake_order
```


```{r}
findAssocs(fake_dtm, "trump",0.4)
findAssocs(fake_dtm, 'much', 0.4)
findAssocs(fake_dtm, 'president', 0.4)
findAssocs(fake_dtm, 'people', 0.4)
findAssocs(fake_dtm, 'state', 0.4)
```


```{r}
true_dtm_order <- order(true_dtm_frequency, decreasing = TRUE)
true_order <- true_dtm_frequency[head(true_dtm_order, n = 20)]
true_order

```


```{r}
findAssocs(true_dtm, "trump",0.4)
findAssocs(true_dtm, 'state', 0.4)
findAssocs(true_dtm, 'reuters', 0.4)
findAssocs(true_dtm, 'president', 0.4)
findAssocs(true_dtm, 'much', 0.4)
```


```{r}
fake_tdm <- as.TermDocumentMatrix(fake_dtm)
fake_tdm <- remove_stopwords(fake_tdm, stopwords = tm::stopwords("english"), min.char = 2,
  max.char = NULL, stem = FALSE, denumber = TRUE)

fake_matrix <- as.matrix(fake_tdm)

fake_sort <- sort(rowSums(fake_matrix), decreasing = TRUE)

fake_df <- data.frame(word = names(fake_sort), freq = fake_sort)
head(fake_df)

fake_dtm_frequency_plot <- ggplot(fake_df[1:20, ], aes(x = reorder(word, -freq), y = freq, fill = word)) + geom_bar(stat = "identity") + 
  labs(x = "Words", y = "Frequency", title = "Most Frequent Words in Fake News")

fake_dtm_frequency_plot
```


```{r}
true_tdm <- as.TermDocumentMatrix(true_dtm)
true_tdm <- remove_stopwords(true_tdm, stopwords = tm::stopwords("english"), min.char = 2,
  max.char = NULL, stem = FALSE, denumber = TRUE)

true_matrix <- as.matrix(true_tdm)

true_sort <- sort(rowSums(true_matrix), decreasing = TRUE)

true_df <- data.frame(word = names(true_sort), freq = true_sort)
head(true_df)

true_dtm_frequency_plot <- ggplot(true_df[1:20, ], aes(x = reorder(word, -freq), y = freq, fill = word)) + geom_bar(stat = "identity") + 
  labs(x = "Words", y = "Frequency", title = "Most Frequent Words in True News")

true_dtm_frequency_plot
```





The number of offensive words usages in both datasets


```{r}
url <- "https://www.cs.cmu.edu/~biglou/resources/bad-words.txt"
offensive_words <- read.delim(url)
head(offensive_words)
```


```{r}
offensive_words<- offensive_words %>%
  mutate(word = offensive_words$abbo) %>%
  mutate(number = 1) %>%
  select(-abbo)
```


```{r}
fake_words = data.frame(text=fake$cleaned_text, stringsAsFactors = FALSE) %>% unnest_tokens(word, text)

head(fake_words)
```


```{r}
fake_compare_offense = inner_join(fake_words, offensive_words, by = "word") 
fake_offense_freq = fake_compare_offense %>%
  count(word) %>%
  arrange(desc(n))
```


```{r}
fake_offense_short_freq <- head(fake_offense_freq, 20)
```


```{r}
dim(fake_offense_freq)
```


```{r}
sum(fake_offense_freq$n)
```


```{r}
ggplot(fake_offense_short_freq, aes(x= reorder(word, -n), y= n, fill = word)) + geom_col() + 
  labs(x = "Offensive Words", y = "Frequency", title = "Most Offensive Words in Fake News")
```


```{r}
true_words = data.frame(text=true$cleaned_text, stringsAsFactors = FALSE) %>% unnest_tokens(word, text)

head(true_words)
```


```{r}
true_compare_offense = inner_join(true_words, offensive_words, by = "word") 
true_offense_freq = true_compare_offense %>%
  count(word) %>%
  arrange(desc(n))
```


```{r}
true_offense_short_freq <- head(true_offense_freq, 20)
```


```{r}
dim(true_offense_freq)
```


```{r}
sum(true_offense_freq$n)
```


```{r}
ggplot(true_offense_short_freq, aes(x= reorder(word, -n), y= n, fill = word)) + geom_col() + 
  labs(x = "Offensive Words", y = "Frequency", title = "Most Offensive Words in Real News")
```


```{r}
fake_readability <- textstat_readability(fake$cleaned_text, c("meanSentenceLength","meanWordSyllables", "Flesch.Kincaid", "Flesch"), remove_hyphens = TRUE,
  min_sentence_length = 1, max_sentence_length = 10000,
  intermediate = FALSE)
head(fake_readability)
```


```{r}
summary(fake_readability)
```


```{r}
fake_read_score <- describe(fake_readability)
```


```{r}
true_readability <- textstat_readability(true$cleaned_text, c("meanSentenceLength","meanWordSyllables", "Flesch.Kincaid", "Flesch"), remove_hyphens = TRUE,
  min_sentence_length = 1, max_sentence_length = 10000,
  intermediate = FALSE)
head(true_readability)
```


```{r}
summary(true_readability)
```


```{r}
true_read_score <- describe(true_readability)
```


```{r}
fake2 <- t(fake_read_score)
true2 <- t(true_read_score)
```


```{r}
fake2 <- data.frame(fake2, row.names = rownames(fake2))
true2 <- data.frame(true2, row.names = rownames(true2))
```


```{r}
write_xlsx(fake2, "~/Documents/2021KCLsem2/Dissertation/kings_project/fake")
write_xlsx(true2, "~/Documents/2021KCLsem2/Dissertation/kings_project/true")
```


